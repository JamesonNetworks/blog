<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-type" content="text/html; charset=utf-8"/>
    <title><%= htmlWebpackPlugin.options.title %></title>
    <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-12722056-2', 'auto');
      ga('send', 'pageview');

      var entry = {};

    </script>

    <script type="application/javascript">

      var entries = JSON.parse('[{\"date\":\"1334116800000\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"I\u2019ve been off and busy working on different projects so I\u2019ve taken an extended departure from my Project Euler conquests in order to work on some other side projects in addition to the day job and school work. Currently, I\u2019m working on a python script to scan a text file and pull out a few relevant tidbits of information. I\u2019m mostly concerned with Author, Title, and Release Date at the moment. My goal is to be able to take an text based ASCII encoded eBook and add it to a database referenced by the title, name, and file path (And anything else I can scan it for). To accomplish this goal, I\u2019m creating a python script that will scan the first 120 lines of the book and then search for specific keywords like Author, Title, etc. When it finds them, it will marshall those entries into an XML file for easy use by other programs. A bash script will traverse the directory structure to find the book, check the format, unzip if required, and then scan the book title and append to an XML file the description of the book.\",\"type\":\"paragraph\"},{\"content\":\"In the course of trying to accomplish this, I\u2019ve found Python to truly be a breeze to work with. It\u2019s looking better and better as a text scanning choice. In a very short time I was able to get the scanner program reading in the first 120 lines of the text file and looking for the \u201Cauthor:\u201D keyword:\",\"type\":\"paragraph\"},{\"content\":\"#! /usr/bin/python\\nimport re\\nimport string\\nfrom shutil import copyfile\\n\\ndef checkline_text( line ):\\n        myscan \\u003d re.compile(r\\\"author: [A-z]* [A-z]*\\\",re.IGNORECASE)\\n        m \\u003d myscan.match(line)\\n        if m is not None:\\n                print m.group()\\n\\ndef checkbook_text( sourcebook, destination ):\\n        copyfile (sourcebook, destination)\\n\\n        i \\u003d 0\\n\\n        openbook \\u003d open(destination)\\n        for line in openbook.readlines():\\n                if i \\u003d\\u003d 120:\\n                        break\\n                s \\u003d line\\n                checkline_text(s)\\n\\n                i \\u003d i + 1\\n\\ndef main():\\n        checkbook_text(\\\"book.txt\\\",\\\"analyzed_book.txt\\\")\\n\\nif __name__ \\u003d\\u003d \\\"__main__\\\":\\n        main()\",\"type\":\"code\"},{\"content\":\"I think the checkbook_text function is pretty self explanatory, but I want to go into more detail about the re.compile statement in the checkline function.\",\"type\":\"paragraph\"},{\"content\":\"Python uses an Re object to represent regular expression searches. The re.compile statement is actually putting together an object that can then be used as a search function. The second line sets m equal to the search object run against the line of text that came into the function. Now, the m object can be analyzed to see if anything useful came out. The if statement is required, because if no match is found, the object comes out as a Python \u201CNone\u201D object, which at this point I\u2019m assuming is like a null, but I didn\u2019t dig too far into it. The code in it\u2019s current form will print out the author name in the first lines of text, but I\u2019m working on getting it to do something more useful. Not bad for about a 20 minute foray into the world of Python. I\u2019m looking forward to expanding on what it does.\",\"type\":\"paragraph\"}],\"title\":\"\"}]},\"subtitle\":\"Posted on April 11, 2012 by Brent\",\"title\":\"Using the Python Re Object\"},{\"date\":\"1336536000000\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"Continuing on the path of trying different languages and working with different aspects of programming, my latest endeavor has involved programming a simple GUI to interact with a utility I\u2019m creating for our work to manage a file that is spread across the network. We have a central location that we want to store the file at, and keep multiple copies on each PC. The problem is seeing if the files are synced across the network, and if they aren\u2019t synced, looking at them to determine what the difference between the \u201Cmaster\u201D file and the remote file is. We need to be able to do this because the files hold configuration specific information for the computer, but a user may have changed the configuration to accomplish a task. Clobbering their configuration without understanding it would be a step in the wrong direction. Currently, we use scripts to push out the latest copy of the \u201Cmaster\u201D file, and backup the previous copy. However, if someone executes the script twice, it will overwrite configuration information. I\u2019ll probably be writing a blog post in the future and open sourcing the utility, but we\u2019ll see about that depending on how the code turns out.\",\"type\":\"paragraph\"},{\"content\":\"In the meantime, the first hurdle to accomplishing this task is creating the way for a user to define a list of computers they wish to monitor and then passing that list back into the main program to actually do the processing. I read a tutorial on passing information between forms with event handlers and found it a little difficult to wade through, so I\u2019m reiterating the process here with a little bit more simplified code to get a handle, in my own mind, of how it works. Please feel free to leave comments if this is the wrong way to approach this problem, it\u2019s my first time really digging into the C# event structure and really my first time working on a GUI-based application. I\u2019m used to writing scripts and command line utilities.\",\"type\":\"paragraph\"},{\"content\":\"The event model of Windows forms seems to rely on three things to accomplish event handling. The first is a parent form which subscribes to the handler of a child form. The second is the child form. The third is the arguments that get passed between the forms in an event arguments object. In this example, I\u2019m going to use some very simple components to move data back and forth: a delegate, an event, an event handler, and a custom class of event arguments. The first thing to look at is the event arguments to be passed between the forms:\",\"type\":\"paragraph\"},{\"content\":\"    public class DataUpdateEventArgs : EventArgs\\n    {\\n        private string data;\\n\\n        public string Data\\n        {\\n            get { return data; }\\n            set { data \\u003d value; }\\n        }\\n\\n        public DataUpdateEventArgs(string nData)\\n        {\\n            this.data \\u003d nData;\\n        }\\n    }\",\"type\":\"code\"},{\"content\":\"This code shows a class named DataUpdateEventArgs which allows the passing of a string into the event object. This is the object we will use to extract the data passed by the form that takes user input. It\u2019s fairly straight forward.\",\"type\":\"paragraph\"},{\"content\":\"The next step is to add an event handler that uses these arguments along with a delegate for the event to the event that is going to be subscribed to by the parent form.\",\"type\":\"paragraph\"},{\"content\":\"        public delegate void DataUpdateHandler(\\n            object sender, DataUpdateEventArgs e);\\n\\n        public event DataUpdateHandler DataUpdated;\",\"type\":\"code\"},{\"content\":\"In this code, we\u2019ve added the delegate handler and the event \u2018DataUpdated\u2019. For the delegate event, we are passing our custom EventArgs extended class DataUpdateEventArgs to include the custom string we are intended to pass. The next step is to fire off the event when our button is clicked and take the data from the component TextBox1 and pass it to the event (in this case the button is named buttonSend):\",\"type\":\"paragraph\"},{\"content\":\"        private void buttonSend_Click(object sender, EventArgs e)\\n        {\\n            string newData \\u003d textBox1.Text;\\n\\n            DataUpdateEventArgs args \\u003d new DataUpdateEventArgs(newData);\\n\\n            DataUpdated(this, args);\\n\\n            this.Dispose();\\n        }\",\"type\":\"code\"},{\"content\":\"So here we create a new DataUpdateEventArg object using a constructor to pass the string into the object, and then change the state of the event DataUpdated for this form along with passing it the new event argument. After that, we dispose of the form.\",\"type\":\"paragraph\"},{\"content\":\"Our form to send data is now publishing an event that a parent form can subscribe to. In our parent form, this text is going to get dumped into a label so we can see the change on the screen:\",\"type\":\"paragraph\"},{\"content\":\"        private void buttonNewForm_Click(object sender, EventArgs e)\\n        {\\n            SendingForm f \\u003d new SendingForm();\\n\\n            f.DataUpdated +\\u003d new SendingForm.DataUpdateHandler(SendingForm_ButtonClicked);\\n\\n            f.Show();\\n\\n        }\\n\\n        public void SendingForm_ButtonClicked(object sender, DataUpdateEventArgs e)\\n        {\\n            lblData.Text \\u003d e.Data;\\n        }\",\"type\":\"code\"},{\"content\":\"Looking at this code, we see two things happening. A form of type SendingForm is created which has the event argument object that our parent form is going to subscribe to. The event is going to be associated with a function included in the parent form called SendingForm_ButtonClicked. This function isn\u2019t tied to any event on the parent form, but instead the code is run when the state of the subscribed event changes. The result will launch a form, allow a user to enter information into a text box. When the user hits save, the form will change the label from lblText to whatever custom text was entered in:\",\"type\":\"paragraph\"}],\"title\":\"\"}]},\"subtitle\":\"Posted on May 9, 2012 by Brent\",\"title\":\"Passing Data Between Forms in C#\"},{\"date\":\"1337140800000\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"So I\u2019m embarking on my first full fledged application that I plan to use in production. The company I work for has a piece of software that relies on files on each client computer in order to import data into our main database from different software packages. In order to achieve this, entries from the individual pieces of software need to be matched up to the names of entries as they exist in our main database. These files are called \u201Ccrosstables.\u201D Normally, the crosstables exist on a network share, however this allows people in one department to mangle crosstables from another department. Using Visual Studio 2010 C# Express, I created an application that uses the .NET 4.0 framework, runs an MD5 hash on each of the files located on individual workstations, and compares it to the MD5 hash of a file on the local computer running the manager program. The layout of the software is included in pictures at the end of this entry. The program allows you to build lists of computers and then save and load those lists. I\u2019m particularly proud of the design principles espoused by Brett Victor in the settings tab using a sentence a link label to toggle the custom settings.\",\"type\":\"paragraph\"},{\"type\":\"picture\",\"id\":\"1\",\"altText\":\"Picture 1\",\"fileType\":\"png\"},{\"type\":\"picture\",\"id\":\"2\",\"altText\":\"Picture 2\",\"fileType\":\"png\"}],\"title\":\"\"}]},\"subtitle\":\"Posted on May 16, 2012 by Brent\",\"title\":\"Full Fledged Production Application\"},{\"date\":\"1345694400000\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"Today I\u2019m going to blog about reversing a string using the bash shell. There are several times when I\u2019m working with legacy VBA code for forms in excel that I end up with a variable list like this to populate the form:\",\"type\":\"paragraph\"},{\"content\":\"    Range(\\\"E74\\\").Value \\u003d Me.text1\\n    Range(\\\"E75\\\").Value \\u003d Me.text2\\n    Range(\\\"E76\\\").Value \\u003d Me.text3\",\"type\":\"code\"},{\"content\":\"Now when the user hits \u201COk,\u201D these new values need to populate these excel cells with a list like this:\",\"type\":\"paragraph\"},{\"content\":\"    Me.text1\\u003d Range(\\\"E74\\\").Value\\n    Me.text2\\u003d Range(\\\"E75\\\").Value\\n    Me.text3\\u003d Range(\\\"E76\\\").Value\",\"type\":\"code\"},{\"content\":\"There are probably a plethora of ways to handle this, but since I typically have an SSH session open to my linux server, I decided to go with a low overhead bash shell. The goal of this code is to take in either the top input or the bottom input and produce the reverse result. It\u2019s trivial to accomplish with a short script:\",\"type\":\"paragraph\"},{\"content\":\"#!/bin/bash\\n\\ncat $1|while read line; do\\n    out\\u003d\\\"\\\"\\n    for w in $line; do\\n        out\\u003d\\\"$w $out\\\"\\n    done\\necho $out\\ndone\",\"type\":\"code\"},{\"content\":\"Now I make a text file with the values I want to reverse, and pass that file name as an argument to the script:\",\"type\":\"paragraph\"},{\"content\":\"b@ssh:~/Documents/scripts\\u003e ./reverse.sh test.txt\\nMe.text1 \\u003d Range(\\\"E74\\\").Value\\nMe.text2 \\u003d Range(\\\"E75\\\").Value\\nMe.text3 \\u003d Range(\\\"E76\\\").Value\\nb@ssh:~/Documents/scripts\\u003e\",\"type\":\"code\"}],\"title\":\"\"}]},\"subtitle\":\"Posted on August 23, 2012 by Brent\",\"title\":\"Reversing a Variable Assignment with Bash\"},{\"date\":\"1346040000000\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"For the second year in a row, Sparc hosted their Hackathon: an all day event allowing teams to compete to create applications on mobile platforms and present those applications to a panel of judges at the end of the day. You can read more about the hackathon and Sparc using the links at the end of this article.\",\"type\":\"paragraph\"},{\"content\":\"Basically, starting in the morning on a Saturday, the teams are assembled and ideas are chosen to develop over the course of that day. Twenty eight teams competed in an event marked with free food, ubiquitous technology, and droves of programmers. Experiencing the hackathon has influenced my opinions on software development from both a collaborative and presentational point of view.\",\"type\":\"paragraph\"},{\"content\":\"To facilitate collaboration among the teams, Sparc opened up private repositories on GitHub for each of the teams. Throughout the course of the day, teams committed their code and a \u201Cboard\u201D (TV with a live stat tracker) kept track of the number of lines of code and the commit messages that were being pushed. Git is a fantastic tool for keeping track of revision history and enabling independent work by multiple team members working on the same projects.\",\"type\":\"paragraph\"}],\"title\":\"\"},{\"contents\":[{\"content\":\"Our team used a Dropbox account which hosted our personal Git repository. We shared the same working directory and I would periodically push the files when the time felt right. Since we had a team of three members, it was fairly easy to resist the urge to work on the same files at the same time. Using the dropbox allowed us to have instant feedback on our application as we developed throughout the day. Dropbox proved to be a very powerful collaborative tool.\",\"type\":\"paragraph\"},{\"content\":\"This was my first experience developing a mobile app or really working in the mobile space at all. As such, most of the programming was done by another member of our team (she had experience working with the Sencha framework and PhoneGap, our weapons of choice for this contest) and I spent most of the day customizing the theming of the framework with SASS and Compass, and learning about iOS transitions and how to access them via the Sencha framework. The third member of our team worked on animations for the application. Our team dynamic was very balanced and worked out perfectly for the goals we were attempting to accomplish.\",\"type\":\"paragraph\"},{\"content\":\"The most important part of our development process was Prototyping. Before we started working on the application at all, we drew up several maps of where buttons would be, what they would do, and how our screens would look. We established the flow of the application early on, and periodically revised the flow throughout the day based on our progress. It was such a positive experience that any UI design I do in the future will involve prototyping.\",\"type\":\"paragraph\"}],\"title\":\"Collaboration\"},{\"contents\":[{\"content\":\"The second thing I learned about \u201Ccrash-designing\u201D a piece of software in one day, and what I would almost consider a cardinal rule for any software project where a demonstration needs to be created within 12 hours:\",\"type\":\"paragraph\"},{},{\"content\":\"If the ultimate goal of building the software is presenting the software, start thinking about the presentation first. Essentially, when given 12 hours to code an app, you are coding a presentable user interface with a skeleton of functionality. You want to be able to show people your idea, not your implementation. Implementations, while important to think about from a \u201Cthis would be realistically possible\u201D point of view, should not be the focus of your programming. The idea should be the end game of your interface.\",\"type\":\"paragraph\"},{\"content\":\"I strongly believe that software presentations, specifically highlighting features and putting together convincing and workable demos, should be a larger part of the software engineering curriculum. A great idea when poorly presented becomes will inevitably fade to the back burner of any software engineering firm.\",\"type\":\"paragraph\"},{\"content\":\"Our team, Team Honeybadger, was able to take home the 2nd place prize for applications built with a framework.\",\"type\":\"paragraph\"},{\"content\":\"Overall the Sparc Hackathon 2.0 was a tremendous success! I\u2019m glad that Sparc hosts the event and would like to thank them and everyone who was involved with making the Hackathon possible! I look forward to seeing everyone again at next years Hackathon 3.0!\",\"type\":\"paragraph\"}],\"title\":\"Presentation\"}]},\"subtitle\":\"Posted on August 27, 2012 by Brent\",\"title\":\"The Sparc Hackathon 2.0\"},{\"date\":\"1359522000000\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"It has been a long time since I\u2019ve posted to the blog, but I have been extremely busy with graduate school for just about all of last semester. I\u2019ve toned it down this semester, reduced my coursework, and am ready to get back into some hobbyists projects. In that spirit, I\u2019m going to talk about a project I\u2019ve spent a couple of days on since the new year that involves parsing text out of one system and into another system.\",\"type\":\"paragraph\"},{\"content\":\"One of my tasks for this year has been grabbing data out of our database and creating a new format that is importable by a facility which uses our business as a subcontracted business. These have been termed electronic data deliverables and I\u2019ll refer to them as EDDs for the rest of the post. EDDs sometimes have very unique quarks because different agencies and companies have wildly different ideas of workflows and process internally.\",\"type\":\"paragraph\"}],\"title\":\"\"},{\"contents\":[{\"content\":\"My main accomplishment of last year was the development of a web based system that handles some of the internal inventory requirements of our company. This system has the ability to query our main database through an API provided by the vendor and track where items in our organization and when they need to be disposed of. Since this infrastructure was already in place and built on C# (and everyone was familiar with the system) I decided that this functionality could feature creep its way into a tab in that MVC application. Eventually, I plan to rework the interface to separate the functions and make a more intelligent menu system, but for now, I just need this today.\",\"type\":\"paragraph\"}],\"title\":\"When you have a hammer, everything looks like a nail\"},{\"contents\":[{\"content\":\"Our system has the ability to export data in the form of comma delimited values by default. Using this built in functionality, I extracted the data from the source db and began to build a class that could extract the separate columns and rebuild the resulting file in the desired structure. I created a new controller and view in my MVC application and began to build the view first, allowing the user to submit a file to the controller.\",\"type\":\"paragraph\"},{\"content\":\"In my main application layout I added:\",\"type\":\"paragraph\"},{\"content\":\"@Html.ActionLink(\\\"EDD Creator\\\", \\\"Index\\\", \\\"EDDGenerator\\\")\",\"type\":\"code\"},{\"content\":\"This creates a link which says \u201CEDD Generator\u201D and points to the index action of the EDDGenerator controller. Then in the views folder I created a EDDGenerator subfolder and an index.cshtml view. the first thing I\u2019m going to do is allow the user select the file and submit the file to the process action:\",\"type\":\"paragraph\"},{\"content\":\"This will allow the user to submit their file to my controller. Now I go and add the controller action for \u201CProcess\u201D so that this form will actually do something (there is also an index controller, with the only statement being \u201Creturn View()\u201D:\",\"type\":\"paragraph\"},{\"content\":\"        public ActionResult Index()\\n        {\\n            return View();\\n        }\\n        [HttpPost]\\n        public ActionResult Process()\\n        {\\n        }\",\"type\":\"code\"},{\"content\":\"At this point, I didn\u2019t know what was going to actually happen. I needed to figure out how this controller was going to handle the csv file that had been submitted. I put in a nonsense statement (int i \\u003d 1) and added a break point to investigate the \u201CRequest\u201D element in the watch screen to figure out what to do with the submitted request.\",\"type\":\"paragraph\"},{\"type\":\"picture\",\"id\":\"1\",\"altText\":\"Picture 1\",\"fileType\":\"png\"},{\"content\":\"A little bit of googling later, and I discover that this file needs to be set into a HttpPostedFileBase variable in order to be accessed in the manner I\u2019m after. I cast the file into this variable and pass it into a class that will handle the data manipulation. This declared \u201Cprocessor\u201D will have a collection of a class of type \u201CSample\u201D and the sample has a collection of class type \u201CResult.\u201D I chose this method because of how the data must be show in the final product (a sample becomes a sort of \u2018heading\u2019 row in the final csv and the results are contained under each sample). In order to accomplish this, I decided to create a class and pass the file into the constructor for processing. The collection would be a member of the class to be accessed after the processing was completed.\",\"type\":\"paragraph\"},{\"content\":\"            HttpPostedFileBase csv_file \\u003d Request.Files[\\\"csv_file\\\"];\\n            if (csv_file !\\u003d null)\\n            {\\n                Processor fileProcess \\u003d new Processor(csv_file);\\n            }\",\"type\":\"code\"},{\"content\":\"I verified this was working and that the file was getting passed into the class and then started the process of breaking down the text. In the processor class, I save the downloaded file to a directory on the server:\\n(In the constructor of Processor.cs)\",\"type\":\"paragraph\"},{\"content\":\"        Samples \\u003d new List();\\n        var fp \\u003d Path.Combine(HttpRuntime.AppDomainAppPath, \\\"ImportUploads/\\\", Path.GetFileName(uploadFile.FileName));\\n        try\\n        {\\n            uploadFile.SaveAs(fp);\\n        }\\n        catch (IOException ex)\\n        {\\n            System.IO.IOException e \\u003d new IOException(\\\"Problem uploading \\\" + uploadFile.FileName + \\\" \\\" + ex.Message, ex);\\n            throw e;\\n        }\",\"type\":\"code\"},{\"content\":\"and begin to read in lines of the file (notice that I throw the exception from this class which I will catch in the process controller should anything go wrong):\",\"type\":\"paragraph\"},{\"content\":\"        var fileIn \\u003d new FileInfo(fp);\\n        var reader \\u003d fileIn.OpenText();\\n        var tfp \\u003d new TextFieldParser(reader) { TextFieldType \\u003d FieldType.Delimited, Delimiters \\u003d new[] { \\\",\\\" } };\\n        int i \\u003d 0;\\n        String[] headers \\u003d tfp.ReadFields();\\n\\n        while (!tfp.EndOfData)\\n        {\",\"type\":\"code\"},{\"content\":\"The first line of my CSV is processed out into a headers object to make it easy to figure out the index of the pieces of data which I need later on. Adding a break point at the while line allows me to investigate the headers array of strings and I use the index of the array to access information in later lines. The ReadFields() method will also move the cursor to the next line, allowing me to exclude the headers in the processed file (where they do not need to be). Inside of the while loop, I read the lines of data into an array of strings and use const ints as indexes for the desired information (this is the fastest way to accomplish my goal, but I\u2019ll talk about improving it later).\",\"type\":\"paragraph\"},{\"content\":\"            String[] parts \\u003d tfp.ReadFields();\\n            Sample thisSample \\u003d new Sample();\\n            thisSample.DATA1 \\u003d parts[INDEXOFDATA1];\\n            thisSample.DATA2 \\u003d parts[INDEXOFDATA2];\\n...snip...\\n            Samples.Add(thisSample);\",\"type\":\"code\"},{\"content\":\"and then do the same thing for results. In my source datafile, all the data I need is on every single line. This is a constraint of the export format from the original database. To get around this, I process and sample and result object on every single line, however I only add the sample to the collection if it is a unique sample. When processing a result, I use a piece of information that the sample and result share to locate the proper sample to add the result to. Each sample object has a collection of result objects as a list. At the end of the process I close the file:\",\"type\":\"paragraph\"},{\"content\":\"tfp.Close();\",\"type\":\"code\"},{\"content\":\"Now, in my controller, I have a properly formed collection of all the samples and results that were in the source csv file. I process this collection, build a long string of all the samples and results, and then return them as a file output stream download:\",\"type\":\"paragraph\"},{\"content\":\"                    foreach (Sample s in fileProcess.Samples)\\n                    {\\n                        ViewBag.Samples +\\u003d i + \\\", \\\" s.ToString() + \\\"\\\\n\\\";\\n                        i++;\\n                        foreach (Result r in s.Results) \\n                        {\\n                            ViewBag.Samples +\\u003d i + \\\", \\\" + r.ToString() + \\\"\\\\n\\\";\\n                            i++;\\n                        }\\n                    }\\n                    ViewBag.Result \\u003d \\\"Success!!\\\";\\n                    string reportString \\u003d ViewBag.Samples;\\n                    string sampleName \\u003d \\\"-\\\" + fileProcess.Samples.ElementAt(0).Name + \\\"-\\\";\\n\\n                    return new WebDisposalTool.Models.FileStringResult(reportString, \\\"application/text\\\"){ FileDownloadName \\u003d \\\"FIN_CSV_\\\" + sampleName + DateTime.Now.ToLocalTime().ToString() + \\\".csv\\\" };\",\"type\":\"code\"},{\"content\":\"Or at the end of the controller if something has gone wrong, I catch any exceptions thrown and return those in the viewbag to a view instead (if you are unfamiliar with C# MVC, the ViewBag is a dynamic variable (might be wrong nomenclature) that is evaluated at run time allowing you to store information that you want to pass to the view from the controller).\",\"type\":\"paragraph\"},{\"content\":\"            catch (IOException e)\\n                {\\n                    ViewBag.Result \\u003d e.Message;\\n                }\\n            }\\n\\n            return View();\",\"type\":\"code\"},{\"content\":\"I created a view.cshtml in the EDDGenerator view folder also to render this error for the user should it ever occur.\",\"type\":\"paragraph\"},{\"content\":\"Here is the entire controller:\",\"type\":\"paragraph\"},{\"content\":\"        [HttpPost]\\n        public ActionResult Process()\\n        {\\n            HttpPostedFileBase csv_file \\u003d Request.Files[\\\"csv_file\\\"];\\n            int i \\u003d 1;\\n\\n            if (csv_file !\\u003d null)\\n            {\\n                try\\n                {\\n                    Processor fileProcess \\u003d new Processor(csv_file);\\n\\n\\n                    foreach (Sample s in fileProcess.Samples) \\n                    {\\n                        ViewBag.Samples +\\u003d i + \\\", \\\" + s.ToString() + \\\"\\\\n\\\";\\n                        i++;\\n                        foreach (Result r in s.Results) \\n                        {\\n                            ViewBag.Samples +\\u003d i + \\\", \\\" + r.ToString() + \\\"\\\\n\\\";\\n                            i++;\\n                        }\\n                    }\\n                    ViewBag.Result \\u003d \\\"Success!!\\\";\\n                    string reportString \\u003d ViewBag.Samples;\\n                    string sampleName \\u003d \\\"-\\\" + fileProcess.Samples.ElementAt(0).Name + \\\"-\\\";\\n\\n                    return new WebDisposalTool.Models.FileStringResult(reportString, \\\"application/text\\\"){ FileDownloadName \\u003d \\\"FILE-\\\" + sampleName + DateTime.Now.ToLocalTime().ToString() + \\\".csv\\\" };\\n\\n                }\\n                catch (IOException e)\\n                {\\n                    ViewBag.Result \\u003d e.Message;\\n                }\\n            }\\n\\n            return View();\\n        }\",\"type\":\"code\"},{\"content\":\"And here is the entire processor class:\",\"type\":\"paragraph\"},{\"content\":\"using System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Web;\\nusing System.Web.Mvc;\\nusing System.IO;\\nusing Microsoft.VisualBasic.FileIO;\\n\\n/// \\n\\n/// Summary description for Processor\\n/// \\n\\npublic class Processor\\n{\\n    public List Samples;\\n\\n    //Sample Indexes\\n    const int DATAINDEX1 \\u003d 38;\\n    const int DATAINDEX2 \\u003d 105;\\n    ...snip....\\n    const int DATAINDEX132 \\u003d 927;\\n\\n    public Processor(HttpPostedFileBase uploadFile)\\n    {\\n        Samples \\u003d new List();\\n        var fp \\u003d Path.Combine(HttpRuntime.AppDomainAppPath, \\\"ImportUploads/\\\", Path.GetFileName(uploadFile.FileName));\\n        try\\n        {\\n            uploadFile.SaveAs(fp);\\n        }\\n        catch (IOException ex)\\n        {\\n            System.IO.IOException e \\u003d new IOException(\\\"Problem uploading \\\" + uploadFile.FileName + \\\" \\\" + ex.Message, ex);\\n            throw e;\\n        }\\n\\n        var fileIn \\u003d new FileInfo(fp);\\n        var reader \\u003d fileIn.OpenText();\\n        var tfp \\u003d new TextFieldParser(reader) { TextFieldType \\u003d FieldType.Delimited, Delimiters \\u003d new[] { \\\",\\\" } };\\n        int i \\u003d 0;\\n        String[] headers \\u003d tfp.ReadFields();\\n\\n        while (!tfp.EndOfData)\\n        {\\n            // Read in all of the fields from the first line of the file\\n            String[] parts \\u003d tfp.ReadFields();\\n\\n                // Process the sample\\n                Sample thisSample \\u003d new Sample();\\n                thisSample.DATA1 \\u003d parts[DATAINDEX1];\\n                ...snip...\\n                thisSample.DATA132 \\u003d parts[DATAINDEX132];\\n\\n                Boolean alreadyAdded \\u003d false;\\n                foreach (Sample s in Samples)\\n                {\\n                    if (s.DATA1.CompareTo(thisSample.DATA1) \\u003d\\u003d 0)\\n                    {\\n                        alreadyAdded \\u003d true;\\n                    }\\n\\n                }\\n\\n                if (!alreadyAdded)\\n                {\\n                    Samples.Add(thisSample);\\n                }\\n\\n                // Process the result\\n                Result thisResult \\u003d new Result();\\n                thisResult.DATA1 \\u003d parts[DATAINDEX1];\\n                ...snip...\\n                thisSample.DATA132 \\u003d parts[DATAINDEX132];\\n\\n                foreach (Sample s in Samples)\\n                {\\n                    if (s.DATA1.CompareTo(thisResult.DATA1) \\u003d\\u003d 0)\\n                    {\\n                        s.Results.Add(thisResult);\\n                    }\\n\\n                }\\n            }\\n        }\\n\\n        tfp.Close();\\n    }\\n}\",\"type\":\"code\"},{\"content\":\"And finally the file return class which I created and referenced above (this was found from googling also):\",\"type\":\"paragraph\"},{\"content\":\"using System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\nusing System.Web;\\nusing System.Web.Mvc;\\n\\nnamespace WebDisposalTool.Models\\n{\\n    public class FileStringResult : FileResult\\n    {\\n            public string Data { get; set; }\\n\\n            public FileStringResult(string data, string contentType)\\n                : base(contentType)\\n            {\\n                Data \\u003d data;\\n            }\\n\\n            protected override void WriteFile(HttpResponseBase response)\\n            {\\n                if (Data \\u003d\\u003d null) { return; }\\n                response.Write(Data);\\n            }\\n    }\\n}\",\"type\":\"code\"}],\"title\":\"Threading the needle\"}]},\"subtitle\":\"Posted on January 30, 2013 by Brent\",\"title\":\"Parsing Text Web Service 101\"},{\"date\":\"1360126800000\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"Alright. I\u2019m done with MacPorts. It isn\u2019t that I didn\u2019t like it or it didn\u2019t work. It did a great job. The problem I have with it is the difficulty of getting ImageMagick to work with a project I\u2019ve started to play with in ruby on rails. HomeBrew, while not making ImageMagick any easier to deal with, does have some very nice features. The best I\u2019ve found so far is the version control system built in Git (Ports may very well have had this, and I never stumbled across it). With Mac HomeBrew, you can look at all the versions stored in a Git repository using the command \u201Cbrew versions \u201C. Using this command, I was able to check out the version of ImageMagick I was hoping to try with the following commands:\",\"type\":\"paragraph\"},{\"content\":\"~\\u003e brew versions imagemagick\\n6.8.0-10 git checkout 7af6c1e Library/Formula/imagemagick.rb\\n6.7.7-6  git checkout 834ce4a Library/Formula/imagemagick.rb\\n6.7.5-7  git checkout f965101 Library/Formula/imagemagick.rb\\n6.7.1-1  git checkout be8e0ff Library/Formula/imagemagick.rb\\n6.6.9-4  git checkout 4e7c332 Library/Formula/imagemagick.rb\\n6.6.7-10 git checkout 0476235 Library/Formula/imagemagick.rb\\n~\\u003e cd /usr/local\\n/usr/local ((default) master*) ~\\u003e git checkout 834ce4a Library/Formula/imagemagick.rb\\n/usr/local ((default) master*) ~\\u003e brew install imagemagick --disable-openmp --build-from-source\\nWarning: It appears you have MacPorts or Fink installed.\\nSoftware installed with other package managers causes known problems for\\nHomebrew. If a formula fails to build, uninstall MacPorts/Fink and try again.\\n\\u003d\\u003d\\u003e Downloading http://downloads.sf.net/project/machomebrew/mirror/ImageMagick-6.8.0-10.tar.gz\\nAlready downloaded: /Library/Caches/Homebrew/imagemagick-6.8.0-10.tar.gz\\n\\u003d\\u003d\\u003e ./configure --disable-osx-universal-binary --without-perl --prefix\\u003d/usr/local/Cellar/imagemagick/6.8.0-10 --enable-shared --disable-static --without-pango\\n\\u003d\\u003d\\u003e make install\\n\\u003d\\u003d\\u003e Caveats\\nSome tools will complain unless the ghostscript fonts are installed to:\\n  /usr/local/share/ghostscript/fonts\\n\\u003d\\u003d\\u003e Summary\\n  /usr/local/Cellar/imagemagick/6.8.0-10: 1422 files, 45M, built in 2.4 minutes\\n/usr/local ((default) master*) ~\\u003e\",\"type\":\"code\"},{\"content\":\"\u2026and bam! The version of ImageMagick which I wanted is now installed. Now the next trick is getting the gem for Ruby on Rails to compile. I\u2019m using the rmagick gem in order to manipulate some uploaded images via CarrierWave. This has worked fine after following a video from Ryan Bates on RailsCasts.com, however the latest version of rmagick is having a tremendous amount of difficulty compiling now. After just about beating my head against the wall, I finally found this gem of a command that got it to install:\",\"type\":\"paragraph\"},{\"content\":\"PKG_CONFIG_PATH\\u003d/usr/local/lib/pkgconfig gem install rmagick\",\"type\":\"code\"},{\"content\":\"Why it needed this command, I\u2019m not sure. But it\u2019s working and I can go back to my other problem that isn\u2019t working now.\",\"type\":\"paragraph\"}],\"title\":\"\"}]},\"subtitle\":\"Posted on February 6, 2013 by Brent\",\"title\":\"Mac OSX Rails Difficulties and Homebrew\"},{\"date\":\"1374552000000\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"Incrudible is a Node.JS library I\u2019ve worked on to handle some boilerplate database functions. I\u2019ve been busy at work at a new job and gearing back up for school. I\u2019m looking forward to the Sparc Hackathon this year and wanted to get some code to be consumed out there. I hope incrudible can help someone else in their experiments with Node.JS\",\"type\":\"paragraph\"},{\"content\":\"https://github.com/JamesonNetworks/incrudible\",\"type\":\"paragraph\"},{\"content\":\"Install it with:\",\"type\":\"paragraph\"},{\"content\":\"npm install incrudible\",\"type\":\"paragraph\"}],\"title\":\"\"}]},\"subtitle\":\"Posted on July 23, 2013 by Brent\",\"title\":\"Introducing Incrudible\"},{\"date\":\"1386824400000\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"Every now and then you have epiphany moments during software development. My epiphany moment has come while working with javascript and attempting to write a frontend for a pet project of mine, http://www.semipho.com.\",\"type\":\"paragraph\"},{\"content\":\"I want to create a website which allows users to share their galleries with specific people that they choose to. Plenty of these sites exist already, but I have some unique idea that I think will set my site apart. In creating the ui for Semipho, I was able to quickly get a front end up in going in about 6 hours. I was happy with my work, and went about implementing new backend features. I was able to get gallery sharing working on the backend and set about creating the appropriate ui hooks. This is where I ran into a problem.\",\"type\":\"paragraph\"},{\"content\":\"After several hours of battling through my terribly muddled ui code, I decided to scrap the whole thing and start rewriting it. Development on Semipho stalled during this time while I worked to come up with an appropriate design pattern to handle my front end. The pattern I\u2019ve decided on is a UI-Bilayer.\",\"type\":\"paragraph\"},{\"content\":\"The UI-Bilayer describes a way of interacting with a RESTful back end in order to promote code sharing between UI \u2018views\u2019. I\u2019m creating a view as a way of looking at an aspect of an application. For Semipho, the index view serves as a way for viewing photos in a person\u2019s gallery of images. All of the html content is segregated into two files, an index.html file and an index_templates.html file. Then, two javascript files control the interaction with the front end. The \u2018view\u2019 javascript file manipulates all of the markup and the DOM and the \u2018core\u2019 javascript file handles interacting with the service. Multiple view files can call into the core file in order to interact with the service, and the core module triggers events on a defined element in the view. Using this design, a single core file can be responsible for all interactions with a service, and the view file can respond accordingly. Decoupling all markup and building an event driven user interface allows the developer to build a UI which is easily extensible by adding new view functions and new events to interact.\",\"type\":\"paragraph\"},{\"content\":\"I\u2019m using Bootstrap, jQuery, jQuery-ui, and Mustache to quickly build robust front ends.\",\"type\":\"paragraph\"},{\"content\":\"I\u2019m going to work on a proof of concept and create a demo on github which will demonstrate the design pattern.\",\"type\":\"paragraph\"}],\"title\":\"\"}]},\"subtitle\":\"Posted on December 12, 2013 by Brent\",\"title\":\"A New Frontend Paradigm\"},{\"date\":\"1394893880503\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"Nirodha (in the context of the four truths in Buddhism) is the absence of suffering. I\\u0027ve created a project by the same name which aims to reduce the suffering that is arranging javascript libraries and CSS files for developing websites. I\\u0027m tired of maintaining source maps and keeping track of whether I\\u0027ve deployed this minified file or that uncompressed js files. I wanted a system in which I could dynamically insert libraries and style sheets, debug them easily, and then deploy minified versions of the final product. This is what Nirodha does.\",\"type\":\"paragraph\"},{\"content\":\"Nirodha is a command line utility with 3 main switches for creating and managing projects (a project is typically one website and views inside of the project are the html pages). Nirodha can create projects and views, serve up the assets for those views, and deploy those views into compiled sites with minified css and js. The project is written in node.js and hosted on Github at https://github.com/JamesonNetworks/Nirodha.\",\"type\":\"paragraph\"}],\"title\":\"\"},{\"contents\":[{\"content\":\"To use nirodha, check out the repository, run npm update to get the required libraries, and then add that directory to your path environment variable. Then, copy the settings_template.json file into settings.json and set the \\u0027path_to_nirodha\\u0027 variable to the nirodha path. Nirodha requires that node and npm be installed order to work. Now, let\\u0027s take a look at the structure of a project!\",\"type\":\"paragraph\"}],\"title\":\"Using Nirodha\"},{\"contents\":[{\"content\":\"A nirodha project contains two directories, \\u0027custom\\u0027 and \\u0027deploy\\u0027. The Custom directory is where you can drop all of your custom libraries for a particular project. CSS and js files in this directory will be compiled and minified when referenced in the project. The \\u0027static\\u0027 folder will be served as if all its files were in the root of the webserver directory. The templates file is for offloading your html templates which you reference in the main html view (I typically use Mustache templates). In the root of the project are the view html files, describing json files which contain all of the library references for a project, and the info.json file which is for Nirodha\\u0027s internal use. When you are developing a web page and want to add a new js library to the page, simply open the $view.json file and add the reference to the appropriate place. The references are arranged in an array form and have a \\u0027title\\u0027 and \\u0027libs\\u0027 property. The title property is used as a place holder to inject the references in the web site (so you can have multiple library sets in given page which allows you to load the js/css either before the page loads completely or after it loads). For instance, if your title was {pre-load}, in your html page you can put the {pre-load} token in the head section of the site and this token will be replaced with the appropriate library references on serving and deploying the page. Take a look at the example project to get a feel for how this works.\",\"type\":\"paragraph\"},{\"content\":\"Creating a project with nirodha is as simple as running: \",\"type\":\"paragraph\"},{\"content\":\"\\u0027nirodha -c $projectname\\u0027\",\"type\":\"code\"},{\"content\":\"The created project will have the default index.html view and associated files. In addition to custom js and css for individual projects, there is also a libs folder in the nirodha root. This folder can hold all of your shared libraries between different sites and references can then be made to the files here as well. So for standard libraries that you will want to use in just about every site, just drop them into the libs folder and then create a reference in your project, nirodha will do the rest.\",\"type\":\"paragraph\"}],\"title\":\"Structure\"},{\"contents\":[{\"content\":\"After you have your references for a project set, you can serve the page using the:\",\"type\":\"paragraph\"},{\"content\":\"\\u0027nirodha -s\\u0027\",\"type\":\"code\"},{\"content\":\"command. This will require sudo unless you set the port higher, but as a default it is set at 80. This runs a webserver that will serve all of the assets as listed in the project. On each page load, the libraries are read and served up. This allows you to develop using the uncompressed js files.\",\"type\":\"paragraph\"}],\"title\":\"Serving your site\"},{\"contents\":[{\"content\":\"Once you are done developing the site, you can run:\",\"type\":\"paragraph\"},{\"content\":\"\\u0027nirodha -d $view\\u0027\",\"type\":\"code\"},{\"content\":\"This deploys the view for your application, reading in all of the referenced js and css, putting them into new files, and compiling and minifying them. The references will be added to the site in the form of \\u0027$view-$token.js\\u0027 or \\u0027$view-$token.css\\u0027. As a final product you will have a website with the smallest number of outside references possible.\",\"type\":\"paragraph\"},{\"content\":\"I\\u0027m using this project to build and manage http://www.semipho.com. If you view the source on semipho, you will see I have two sections of libraries referenced, the pre-load and post-load sections which generate 5 files overall, a js and css for before loading, an html file with all of the templates injected, and a js and css after loading in init anything that needs to be setup after the DOM has loaded. This allows me the convenience of developing using all uncompressed libraries and deploying a project which only has 5 required connections to the server to get the smallest possible CSS and JS files!\",\"type\":\"paragraph\"},{\"content\":\"I\\u0027ve released this project under the MIT license and hope that you find it useful to use in your projects!\",\"type\":\"paragraph\"}],\"title\":\"Deploying your site\"}]},\"subtitle\":\"Posted on Saturday, March 14th 2014 by Brent\",\"title\":\"Nirodha - Simple JS and CSS management for websites\"},{\"date\":\"1399780800000\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"It\\u0027s been a long time since my last post, but I\\u0027m finally done with exams and working on projects again. In the past, these postings have been hosted on a wordpress blog, however, I\\u0027ve been seeing nice looking blogs that are more performant and stylish. I felt compelled to build something a little more custom to suit the changes in the blogging format. So, using Nirodha, I\\u0027ve built a UI around JSON entries and created a node app in order to serve those entries. With this post, I introduce my new blogging platform: JournalJS. \",\"type\":\"paragraph\"}],\"title\":\"\"},{\"contents\":[{\"content\":\"JournalJS is an open source blogging platform hosted on Github. It can be accessed at https://github.com/JamesonNetworks/JournalJS. Currently, I\\u0027m writing my posts into JSON blocks that are processed by a file adapter called the jsonFiles adapter. I look forward to anyone who wants to use JournalJS forking the repository and writing their own adapters for whatever blogging backends they are currently using!\",\"type\":\"paragraph\"}],\"title\":\"JournalJS\"},{\"contents\":[{\"content\":\"The adapter has two methods, entries() and get_entry(). With the json adapter, I\\u0027m just serving some static json files as the entries. However, if you want to write an adapter for any other medium, you can grab the post that you want and massage it the following template format:\",\"type\":\"paragraph\"},{\"content\":\"{\\n    \\\"title\\\": \\\"template\\\",\\n    \\\"subtitle\\\": \\\"\\\",\\n    \\\"date\\\": \\\"\\\",\\n    \\\"markdown\\\" : \\n    { \\n        \\\"sections\\\": [\\n            {\\n                \\\"title\\\": \\\"\\\",\\n                \\\"contents\\\" : \\n                [\\n                    {\\n                        \\\"type\\\": \\\"paragraph\\\",\\n                        \\\"content\\\": \\\"\\\"\\n                    },\\n                    {\\n                        \\\"type\\\": \\\"code\\\",\\n                        \\\"content\\\": \\\"\\\"\\n                    },\\n                    {\\n                        \\\"type\\\": \\\"picture\\\",\\n                        \\\"content\\\": \\\"\\\"\\n                    }\\n                ]\\n            }\\n        ]\\n    }\\n}\",\"type\":\"code\"},{\"content\":\"The following code retrieves the list of the files from the filesystem to serve to the front end:\",\"type\":\"paragraph\"},{\"content\":\"jsonFiles.prototype.entries \\u003d function(current_count) {\\n\\tvar files \\u003d fs.readdirSync(process.cwd() + conf.entries);\\n\\tvar entries \\u003d [];\\n\\tfor(var i \\u003d 0; i \\u003c files.length; i++) {\\n\\t\\tif(JSON.stringify(files[i]).indexOf(\\u0027.json\\u0027) \\u003e 0) {\\n\\t\\t\\tvar entry \\u003d fs.readFileSync(\\u0027./\\u0027 + conf.entries + \\u0027/\\u0027 + files[i], \\u0027utf-8\\u0027);\\n\\t\\t\\tvar jsonEntry \\u003d JSON.parse(entry);\\n\\t\\t\\t//console.log(JSON.stringify(jsonEntry));\\n\\t\\t\\tif(jsonEntry.title !\\u003d\\u003d \\u0027template\\u0027) {\\n\\t\\t\\t\\tentries.push(jsonEntry);\\t\\t\\t\\n\\t\\t\\t}\\t\\t\\t\\n\\t\\t}\\n\\t}\\n\\treturn entries.reverse();\\n}\",\"type\":\"code\"},{\"content\":\"And I\\u0027m redirecting the direct get entry with the following code in the server given that we\\u0027re using the jsonFiles adapter:\",\"type\":\"paragraph\"},{\"content\":\"if(conf.adapter \\u003d\\u003d\\u003d \\u0027jsonFiles\\u0027) {\\n\\trouter.use(\\u0027/\\u0027, express.static(__dirname + conf.entries));\\t\\n}\\nelse {\\n\\trouter.use(\\u0027/\\u0027, function(req, res) {\\n\\t\\tvar adapter \\u003d require(\\u0027./bin/\\u0027 + conf.adapter);\\n\\t\\tvar entry \\u003d adapter.get_entry(req.url);\\n\\t});\\n}\",\"type\":\"code\"},{\"content\":\"So feel free to fork JournalJS and post any issues you run into on Github!\",\"type\":\"paragraph\"}],\"title\":\"The Adapter Format\"}]},\"subtitle\":\"Posted on Sunday, May 11th, 2014 by Brent\",\"title\":\"JournalJS - Blogging Built on Nirodha\"},{\"date\":\"1400718164180\",\"markdown\":{\"sections\":[{\"contents\":[{\"type\":\"picture\",\"id\":\"1\",\"altText\":\"Picture 1\",\"fileType\":\"png\"},{\"content\":\"As you can see, I\\u0027ve added image support into JournalJS. These images are responsive thanks to Twitter\\u0027s Bootstrap and they have a format that allows the blog maker to simply index them by number and refer to them as any other content entry. I\\u0027ll go into the details about their format in the JSON blog entry in the following section.\",\"type\":\"paragraph\"}],\"title\":\"\"},{\"contents\":[{\"content\":\"The code for introducing images was fairly simple. I\\u0027m expanding on the existing content system, adding a filter for the content type picture. Then I take the information in the content entry, dump it into an image template with the proper classes, and present it on the page. The picture entries have the format of the date of the entry for the filename plus an index by which to refer to them. The resulting filename looks like:\",\"type\":\"paragraph\"},{\"content\":\"1400718164180_1.png\",\"type\":\"code\"},{\"content\":\"Once the pictures are created, they are served along with the blog entries themselves. I added the filter in the index.js for the blog app:\",\"type\":\"paragraph\"},{\"content\":\"\\tfor(var i \\u003d 0; i \\u003c sections.length; i++) {\\n\\t\\tvar section \\u003d sections[i];\\n\\t\\tfinalHtml +\\u003d Mustache.to_html(sectionHeaderTemplate, section);\\n\\t\\tfor(var k \\u003d 0; k \\u003c section.contents.length; k++) {\\n\\t\\t\\tvar content \\u003d section.contents[k];\\n\\t\\t\\tswitch(content.type) {\\n\\t\\t\\t\\tcase \\u0027paragraph\\u0027:\\n\\t\\t\\t\\t\\tfinalHtml +\\u003d Mustache.to_html(sectionContentParagraphTemplate, content);\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\tcase \\u0027code\\u0027:\\n\\t\\t\\t\\t\\tfinalHtml +\\u003d Mustache.to_html(sectionContentCodeTemplate, content);\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\tcase \\u0027quote\\u0027:\\n\\t\\t\\t\\t\\tfinalHtml +\\u003d Mustache.to_html(sectionContentQuoteTemplate, content);\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\tcase \\u0027picture\\u0027:\\n\\t\\t\\t\\t\\tvar picture \\u003d {};\\n\\t\\t\\t\\t\\tpicture.url \\u003d conf.blogserver + \\u0027/\\u0027 + post.date + \\u0027_\\u0027 + content.id + \\u0027.\\u0027 + content.fileType;\\n\\t\\t\\t\\t\\tpicture.alttext \\u003d content.altText;\\n\\t\\t\\t\\t\\tfinalHtml +\\u003d Mustache.to_html(sectionContentPictureTemplate, picture);\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\tdefault:\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\",\"type\":\"code\"},{\"content\":\"The structure of the pictures in the blog post look like this:\",\"type\":\"paragraph\"},{\"content\":\"{\\n\\t\\\"type\\\": \\\"picture\\\",\\n\\t\\\"id\\\": \\\"1\\\",\\n\\t\\\"fileType\\\": \\\"png\\\",\\n\\t\\\"altText\\\": \\\"Picture 1\\\"\\n}\",\"type\":\"code\"},{\"content\":\"And when we put it all together we can see pictures in our blog post!\",\"type\":\"paragraph\"},{\"type\":\"picture\",\"id\":\"2\",\"altText\":\"Picture 2\",\"fileType\":\"png\"},{\"content\":\"So long and thanks for all the fish!\",\"type\":\"quote\",\"attributedTo\":\"Hitchhiker\\u0027s Guide to the Galaxy\"}],\"title\":\"The Code\"}]},\"subtitle\":\"Posted on May 21, 2014 by Brent\",\"title\":\"Adding image support to JournalJS\"},{\"date\":\"1410642186800\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"This week, I decided to refactor a project of mine called Nirodha. Nirodha is a small utility for creating and deploying websites. It manages javascript and css dependencies so you can simply focus on getting some work done. I\\u0027ve been thinking about expanding on it for a long time, and one thing I was really after was incorporating the bower package manager into it. My plan is to work that new feature after my refactoring is complete.\",\"type\":\"paragraph\"}]},{\"contents\":[{\"content\":\"The first thing I wanted to do was work out the next major features I wanted to introduce into Nirodha. This would give me an idea of the surfaces of the software that needed work. I would know where I need to expand on functionality and where new functionality is being inserted.\",\"type\":\"paragraph\"},{\"content\":\"The first feature that came to mind was the ability to add in bower support. This will affect the directories that Nirodha reads from, and will require adding in some special logic to figure out where the dependency JS and CSS files are located. Doesn\\u0027t sound too bad. The second feature is a \\u0027watch\\u0027 mode. I want to be able to put nirodha into a watch mode, and redeploy the site anytime the file system code changes. This will allow me to more easily incorporate nirodha projects as separate frontend code bases, while allowing the created assets to be served from a standard web server without constantly redeploying the minified files. The third and final feature is allowing multiple views to be deployed at once. This means that running a nirodha -d will deploy all the views of the project, in the event that none are specified.\",\"type\":\"paragraph\"},{\"content\":\"You can almost do this as an exercise. Look at some program and see if there\\u0027s some duplication. Then, without really thinking about what it is you\\u0027re trying to achieve, just pigheadedly try to remove that duplication. Time and time again, I\\u0027ve found that by simply removing duplication I accidentally stumble onto a really nice elegant pattern. It\\u0027s quite remarkable how often that is the case. I often find that a nice design can come from just being really anal about getting rid of duplicated code. \",\"type\":\"quote\",\"attributedTo\":\"Martin Fowler\"}],\"title\":\"Step 1: Collect new features\"},{\"contents\":[{\"content\":\"The most important aspect of this refactoring was test coverage. I believe that if you do not have test coverage, refactoring is much more difficult. Nirodha already has a solid set of tests in place providing test coverage over most of the code base, so I was able to refactor with impunity for the most part. I did run into some issues after the refactoring was complete with features not working. I\\u0027ve updated the tests to address some of those holes.\",\"type\":\"paragraph\"},{\"content\":\"Refactoring to me used to be about removing duplicated code and getting patterns in place where they belong once more is understood about the software product being developed. I still believe these are important goals in refactoring. I also have a new rule: Get all the crazy in one place. I believe that if you have something that is difficult to abstract and ends up being clunky, you should wrap it in a nice class and throw nice exceptions out of it. Just make sure you have some easy way for your project to consume it, so eventually you can either weed out the crazy, or sweep it far enough under the rug that you don\\u0027t have to worry about it. The other advantage of this approach, is that if exceptions come from that layer, you know its going to be bad. This helps you estimate fix times.\",\"type\":\"paragraph\"},{\"content\":\"Get all the crazy in one place.\",\"type\":\"quote\",\"attributedTo\":\"Brent Jameson\"},{\"content\":\"For me, this meant getting everything related to deploying and serving files in either the same place or very close together. Unfortunately, the deploy process and the serve process are very different beasts, so they are still separate. The class that handles deploying (and several other nirodha functions) is the nirodhaManager. This blobby, poorly named mess of a class is all of the crazy that nirodha needs to do. Putting it in the one place gives it a testable surface, and some opportunities for refactoring. All of the duplicated code in that class has been pulled out into functions, and some of it has been pulled out into a utility class. This class is shared by nirodhaManager and libraryManager and puts all of the logic for finding libraries, filtering file types, and deriving the working path of nirodha in one place. This also allowed me to delete the \\u0027path_to_nirodha\\u0027 variable from the settings of nirodha, which makes first time setup less painful. \",\"type\":\"paragraph\"},{\"content\":\"At the end of the nirodha refactoring, I had twenty four test cases and 81% test coverage on Nirodha as a whole.\",\"type\":\"paragraph\"},{\"type\":\"picture\",\"id\":\"1\",\"altText\":\"Test output\",\"fileType\":\"png\"},{\"type\":\"picture\",\"id\":\"2\",\"altText\":\"Statement coverage\",\"fileType\":\"png\"}]}]},\"subtitle\":\"By Brent Jameson\",\"title\":\"The Nirodha Refactoring - Part 1\"},{\"date\":\"1410830266572\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"As I worked on refactoring Nirodha, I noticed something in other repositories that I wanted to copy. Several large projects had badges about test coverage and dependency status and all kinds of interesting information about their code. So, I decided I wanted to get some of those also, and set about making it happen with Nirodha.\",\"type\":\"paragraph\"},{\"content\":\"The first step was to figure out which service providers provided access to the variety of badges. I started with TravisCI, the most common service for continuous integration. I also added Coveralls, Code Climate, and David.io for code coverage, code quality, and dependency status respectively. I set about getting TravisCI integrated and worked on increasing my test coverage.\",\"type\":\"paragraph\"},{\"content\":\"TravisCI required adding a .travis.yml file. This file contained the following:\",\"type\":\"paragraph\"},{\"content\":\"language: node_js\\nnode_js:\\n  - \\\"0.11\\\"\\n  - \\\"0.10\\\"\",\"type\":\"code\"},{\"content\":\"This will tell TravisCI what versions of node to build on. Now that you have github tied into TravisCI and your .travis.yml file populated, when you make commits and pushes to git, your source code will automatically get built and tested.\",\"type\":\"paragraph\"}],\"title\":\"\"},{\"contents\":[{\"content\":\"The next step was to set up David-dm in order to monitor the dependencies listed in package.json and alert me when they get out of date. This was also an extremely easy process, and once the package was published into npm, I could very quickly see which dependencies needed to be updated.\",\"type\":\"paragraph\"},{\"content\":\"See more at https://david-dm.org/JamesonNetworks/Nirodha\",\"type\":\"paragraph\"}],\"title\":\"David-dm\"},{\"contents\":[{\"content\":\"Coveralls will take an lcov printed set of unit tested results, and display the amount of code coverage that you have with your unit tests.\",\"type\":\"paragraph\"},{\"content\":\"See more at https://coveralls.io/r/JamesonNetworks/Nirodha\",\"type\":\"paragraph\"}],\"title\":\"Coveralls\"},{\"contents\":[{\"content\":\"Code Climate was my favorite. Code Climate rates your code and gives it a GPA based on different metrics. It also shows you your code coverage and a line by line break down of what is being tested and what is not. I really think that it was one of the coolest tools I\\u0027ve ever used in software development. I was able to increase my code coverage to 91% using code climate very quickly.\",\"type\":\"paragraph\"},{\"type\":\"picture\",\"id\":\"1\",\"altText\":\"Test output\",\"fileType\":\"png\"},{\"content\":\"After improving that test coverage, I poked around with some of the other things Code Climate provides. It will rate your classes in terms of how well they are abstracted or how complex they are.\",\"type\":\"paragraph\"},{\"type\":\"picture\",\"id\":\"2\",\"altText\":\"Test output\",\"fileType\":\"png\"},{\"content\":\"The feature that I found most helpful was the actual display of all the lines of code and the levels of coverage. It was very helpful in increasing the amount of test coverage I had.\",\"type\":\"paragraph\"},{\"type\":\"picture\",\"id\":\"3\",\"altText\":\"Test output\",\"fileType\":\"png\"},{\"content\":\"See more at https://codeclimate.com/github/JamesonNetworks/Nirodha\",\"type\":\"paragraph\"}],\"title\":\"Code Climate\"},{\"contents\":[{\"content\":\"Finally, here\\u0027s the README.md file. Get a load of all those badges.\",\"type\":\"paragraph\"},{\"type\":\"picture\",\"id\":\"4\",\"altText\":\"Test output\",\"fileType\":\"png\"}],\"title\":\"The final README.md file\"}]},\"subtitle\":\"Pursuing 100% test coverage for Nirodha\",\"title\":\"Open Source Testing Tools\"},{\"date\":\"1464671842228\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"Hello World! It has been a long time since I last created an entry for this blog. As a matter of fact, it has been since September 15th, 2014. During this absence, so much has changed in my life. I\\u0027ve been pursuing several different development tracks, career paths, and living an awful lot of life. I\\u0027ve managed to graduate from The Citadel\\u0027s / College of Charleston\\u0027s joint program with a masters in Computer Science, and now I finally have some free time to start pursuing some of my passion projects again. So, lucky you! I\\u0027m going to resume blogging! \",\"type\":\"paragraph\"},{\"content\":\"My first entry will be coming out this week. My goal will be to produce at least one blog post per week. I do not know whether I will be able to keep up that pace or not, as I\\u0027m planning to do some substantial deep dives into different pieces of technology that I\\u0027m working with. When I left this blog, I was dabbling in C#, node.js, and working on writing a package manager which I called Nirohda much like npm, bower, or as of late webpack. Nirohda never caught on and was arguably never as flexible or powerful as the other package managers, however, it was a spectacular learning experience that really prepared me for future challenges in my career. Today, several of the technologies I use have changed, but the main theme stays the same. Keep it simple!\",\"type\":\"paragraph\"},{\"content\":\"My current technology stack diverges drastically. At my $dayJob, I\\u0027m working as a Magento developer to create eCommerce based solutions. Some of my blog posts will focus on aspects of Magento and the newly minted Magento 2. My first post will be covering aspects of the front end architecture where the documentation is currently lacking. For their front end, Magento 2 heavily leverages knockout and jQuery UI along with the typical Magento-y spin on those technologies. I plan to dive into their initialization of front end assets and step through the process of creating a reusable snippet of javascript. \",\"type\":\"paragraph\"},{\"content\":\"For hobby projects, I\\u0027ve become quite an evangelist of Spring Boot. If you haven\\u0027t tried Spring Boot, I highly suggest pulling down a pom.xml from the spring boot io builder and working on creating a fat jar of a project. Spring Boot really simplifies the process of setting up a Spring project and leveraging the amazing power that Spring offers developers. The initial commit of this blog serves as a great template for a Spring Boot project. For the front end, I\\u0027m currently running Redux/React. Redux is a design paradigm that pushes one directional data flow and represents the UI as a state machine which is acted on by actions and transitions an application state tree in predictable ways. I\\u0027m linking a set of videos that serve as a great introduction to Redux (https://egghead.io/series/getting-started-with-redux). If you haven\\u0027t checked out Redux for front end development, you are truly missing out on a great new world of development. \",\"type\":\"paragraph\"},{\"content\":\"So, it\\u0027s great to be back online and creating articles! I can\\u0027t wait for our deep dive of the Magento 2 front end asset pipeline!\",\"type\":\"paragraph\"}],\"title\":\"Hello World.\"}]},\"subtitle\":\"Back in the saddle again\",\"title\":\"Hello World 2 - The Return of JamesonNetworks Blog\"},{\"date\":\"1464820404012\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"I want to take a look at the eCommerce platform Magento 2 and talk a little about what you can expect from its front end architecture. Magento 2 represents the latest release of an eCommerce platform for the Magento crew. Among the goals for the platform are using a modern tech stack and streamlining customizations. In these veins, Magento 2 has introduced several changes from Magento 1, especially to the front end architecture and libraries they have decided to include by default. jQuery has been chosen over prototype and a new databinding library has been added called Knockout.js. They\\u0027ve also heavily leveraged RequireJs to facilitate the use of AMD. If you\\u0027re been around in front end circles, you know that both jQuery and Knockout have been displaced by other technologies, but this is nothing new for front end development. The bleeding edge is generally orders of magnitude ahead of framework infrastructure and constantly changing. If you follow my blog, you know that I myself would rather see Redux/React fill the niche of front end development, however, we go to war with the army we have. So if you are a Magento 2 developer, I\\u0027m here to run you through the process of making a reusable front end component and get that bit of functionality to display on the front end (and if you\\u0027ve been running in the Magento 2 circles, you know that this is a feat that is easier said than done).\",\"type\":\"paragraph\"},{\"content\":\"The first step in creating a reusable Magento 2 component is to recognize the design paradigms that are being used on the front end. We can boil the components that Magento 2 uses into 3 different categories: Magento Special Sauce components, Knockout components, and jQuery-UI widgets (not to be confused with magento widgets.\",\"type\":\"paragraph\"},{\"content\":\"1) Magento Special Sauce Components : The rule of thumb that I have found is when Magento needs to string components together to build advanced functionality, they tend to use their own components extending off of their custom component class (https://github.com/magento/magento2/blob/develop/app/code/Magento/Checkout/view/frontend/web/js/view/registration.js). This extension allows them to compose these types of components together using a jsLayout object (which seems to me to be analogous to the XML merging they do for layout XML). The jsLayout piece is one of the more confusing aspects and honestly I\\u0027m still trying to wrap my ahead around exactly how and where to use it. For the most part, I\\u0027ve avoided that model in favor of creating jQuery widgets.\",\"type\":\"paragraph\"},{\"content\":\"2) Knockout Components : Knockout components use knockout js to make calls for specific customer information that doesn\\u0027t get cached by Magento. While knockout is all over the place on the Magento front end, I\\u0027ve yet to find something I would consider a canonical example of a knockout component that would make for a good example for this blog post. If you are trying to figure out a problem with knockout and magento, I highly recommend first reading the knockout documentation, then installing the knockout.js plugin for Chrome. This will let you inspect areas in the DOM and instantly see what data is bound to the DOM. I haven\\u0027t messed with knockout components very much for similar reasons the Magento special components, I just don\\u0027t understand it and haven\\u0027t seen an example that gives me that \\u0027a ha\\u0027 moment.\",\"type\":\"paragraph\"},{\"content\":\"3) jQuery-UI widgets: In my mind, the clearest examples of modular pieces of functionality in Magento 2 are derived from the jQuery UI widgets. By integrating with the jQuery UI component lifecycle, Magento 2 provides developers with a very convenient and understandable way to plug their functionality into the front end. An example would be the stock mega menu which leverages jQuery UI and expands on it in order to provide the main navigation menu experience (jQueryUI Base: https://github.com/magento/magento2/blob/develop/lib/web/jquery/jquery-ui-1.9.2.js, Magento custom widget: https://github.com/magento/magento2/blob/develop/lib/web/mage/menu.js). These examples show a widget that is extending off of the UI base widgets and then inserting themselves into the DOM where they need to be. So that brings us to our next step:\",\"type\":\"paragraph\"}],\"title\":\"\"},{\"contents\":[{\"content\":\"I\\u0027m going to dive deeper into the components later on in this post, but for now I want to talk about how you initialize javascript inside of Magento 2 including code examples for what is going on under the hood. Magento 2 has a very unique and Magento specific way to inject javascript on the page. They\\u0027ve introduced a custom DOM attribute called data-mage-init where you can specify a (we\\u0027ll call them) \\\"module\\\" (our jQuery widget) on a particular element on the page. data-mage-init seems to have evolved over time and is represented in the code base in two forms. One is the DOM attribute, the other is a custom script block with the type of \\u003cscript type\\u003d\\\"text/x-magento-init\\\"\\u003e. I\\u0027m pointing this out because you will see it done both ways in Magento core modules and it\\u0027s easy to get confused about what is going on. If you dive into the actual parser for Magento 2\\u0027s js init steps, you\\u0027ll actually see the script block for text/x-magento-init get converted into a data-mage-init DOM attribute before it gets processed (converting script tags: https://github.com/magento/magento2/blob/develop/lib/web/mage/apply/scripts.js, data-mage-init processing: https://github.com/magento/magento2/blob/develop/lib/web/mage/apply/main.js). So how do we use this new Magento attribute?\",\"type\":\"paragraph\"},{\"content\":\"Magento 2 has a short description of what happens for javascript initialization (http://devdocs.magento.com/guides/v2.0/javascript-dev-guide/javascript/js_init.html), but in my opinion they don\\u0027t go into enough detail to really get a Magento developer off of the ground. For this exercise, we are going to assume that you already have a module in place, and that your module is functioning in Magento 2. From here, we need to create a folder called view/frontend/web/js/ inside of our module. This is where we will put our javascript component. We will create a file called click.js. Click is going to handle an element being clicked on, and dispatch an alert to give the user a pop up window that says Hello World. I create my widget using the jQuery UI widget factory and taking advantage of the provided component lifecycle (http://api.jqueryui.com/jQuery.widget/).\",\"type\":\"paragraph\"},{\"content\":\"define([\\u0027jquery\\u0027], function($) {\\r\\n    \\\"use strict\\\";\\r\\n\\r\\n    $.widget(\\u0027ui.click\\u0027, {\\r\\n        options: {\\r\\n        },\\r\\n        _create: function () {\\r\\n            this.element.on(\\u0027click\\u0027, function(event) { alert(\\u0027Hey! I\\\\\\u0027m on teh page!\\u0027) })\\r\\n        }\\r\\n    });\\r\\n\\r\\n    return {\\u0027myClickWidget\\u0027: $.ui.click};\\r\\n});\",\"type\":\"code\"},{\"content\":\"It is important to note the _create method which acts as the widgets constructor and the peculiar way in which we must return our jQuery UI widget. The return signature becomes very important in the data-mage-init attribute. Now that we have our widget created, we have to let Magento 2 know that we have defined a widget that we plan on using before we are able to use the widget. We do this by creating a file in the web folder called \\u0027requirejs-config.js\\u0027.\",\"type\":\"paragraph\"},{\"content\":\"var config \\u003d {\\r\\n    map: {\\r\\n        \\u0027*\\u0027: {\\r\\n            \\u0027myClickWidget\\u0027: \\u0027MyCompany_MyModule/js/click\\u0027\\r\\n        }\\r\\n    }\\r\\n};\",\"type\":\"code\"},{\"content\":\"So, once we have this file, Magento 2 is going to recognize that we have a javascript module and we\\u0027ll be able to use it in our data-mage-init attribute. Whats the trick here? This file\\u0027s contents are going to be included in a full requirejs-config.js. So, if you have a path wrong or a syntax error or something, when Magento goes to build it\\u0027s global file, it will leave yours out. So step one to getting js on the page after creating these files is to check the requirejs-config.js file inside of pub/static/_requirejs (and then digging down to your theme). If you don\\u0027t see it there, try deleting the file and running the content deployment step again. YOUR REQUIREJS FILE MUST APPEAR HERE IN ORDER FOR MAGENTO TO LOAD IT. When you confirm that your entry is there, it\\u0027s time to create a template file and leverage your new widget.\",\"type\":\"paragraph\"},{\"content\":\"In our front end folder I\\u0027m going to make a template called myclicktemplate.phtml. This is the file where I will put my data-mage-init attribute\",\"type\":\"paragraph\"},{\"content\":\"\\u003cdiv\\u003e\\r\\n\\t\\u003cdiv data-mage-init\\u003d\\u0027{\\\"myClickWidget\\\": {} }\\u0027\\u003e\\u003c/div\\u003e\\r\\n\\u003c/div\\u003e\",\"type\":\"code\"},{\"content\":\"While I\\u0027m going to gloss over getting the block on your particular page, there are numerous posts dedicated to that particular task and its very similar in Magento 2 as it is in Magento 1. If you want more information on that, ping me and eventually I\\u0027ll work on getting a post dedicated to getting information from a template to the screen, but in the meantime, we\\u0027ll just pretend that we already did that. Now when I load the page with my widget, a whole bunch of magic is going to happen. Ultimately, what happens is the widget I created gets initialized with this.element equal to the element that I placed the data-mage-init attribute on. Did it work? If it didn\\u0027t, let\\u0027s talk through some of the pitfalls that could be occurring.\",\"type\":\"paragraph\"}],\"title\":\"How the hell do I get my javascript on the page?\"},{\"contents\":[{\"content\":\"Obviously, the first place to look for problems is in Chrome Developer Tools. If you have an exception there, it will give you a great indication of where to start looking. The first pitfall is listed above, files not being included in requirejs-config.js. Assuming that is working, the next place we need to look is inside of main.js (https://github.com/magento/magento2/blob/develop/lib/web/mage/apply/main.js#L22):\",\"type\":\"paragraph\"},{\"content\":\"function init(el, config, component) {\\r\\n    require([component], function (fn) {\\r\\n\\r\\n        if (typeof fn \\u003d\\u003d\\u003d \\u0027object\\u0027) {\\r\\n            fn \\u003d fn[component].bind(fn);\\r\\n        }\\r\\n\\r\\n        if (_.isFunction(fn)) {\\r\\n            fn(config, el);\\r\\n        } else if ($(el)[component]) {\\r\\n            $(el)[component](config);\\r\\n        }\\r\\n    });\\r\\n}\",\"type\":\"code\"},{\"content\":\"Here is the \\u0027flux-capacitor\\u0027 of the data-mage-init attribute. It\\u0027s what makes initializing widgets possible. Looking at this method is a little confusing at first, but basically anything that is passed to data-mage-init ends up here. If you want to dig into it, place a debugger; statement or a breakpoint on this function and take a look. This method understands three method signatures returned from our js file. The first is the one we care about: jQuery-UI. If you investigate these parameters, it becomes clear why the naming and return types of our js file are so important. They will be used here to determine what actually gets loaded! So if you have a problem in any of those places, this is where you can look to try and resolve it.\",\"type\":\"paragraph\"},{\"content\":\"So, there we have it. This is a run down of a design paradigm that can be reused over and over again in developing for the Magento 2 front end. I wrote this post because I don\\u0027t feel that Magento has provided us a clear direction and a canonical example of incorporating front end assets in Magento 2 modules. Using this method has several advantages over simply putting \\u003cscript\\u003e blocks into Magento phtml templates. With our javascript in an independent javascript file, we can now lint the js and keep it separated from the particular template implementation we are looking at.\",\"type\":\"paragraph\"},{\"content\":\"In another post, I\\u0027ll go into some detail about the syntax of data-mage-init and how we can pass properties and settings down to our widget from our php template in order to incorporate framework settings and variables into our js asset.\",\"type\":\"paragraph\"}],\"title\":\"Wait, I don\\u0027t see my javascript. What went wrong?\"}]},\"subtitle\":\"Navigating the Magento 2 front end architecture\",\"title\":\"Magento 2 Front End Deep Dive\"},{\"date\":\"1473354120998\",\"markdown\":{\"sections\":[{\"contents\":[{\"content\":\"Have you ever wanted to override native magento 2 functionality and plug into a certain javascript method without interfering with the normal code? This used to be accomplished using the Prototype function .wrap, which would allow you to wrap any function with your own and allow you to both 1) intercept the arguments going to it and 2) intercept the return value coming out of it. However, with the advent of Magento 2 and it\\u0027s new RequireJS component based structure, it\\u0027s a little less opaque on how you go about intercepting and injecting your own code. It is, however, still entirely possible!\",\"type\":\"paragraph\"},{\"content\":\"In order to accomplish our goal, we are going to look at switching out native M2 shipping rates validation rules with our own. The first step is to setup our requirejs-config.js to accurately reflect our substitutions.\",\"type\":\"paragraph\"},{\"content\":\"var config \\u003d {\\n    map: {\\n        \\u0027*\\u0027 : {\\n            \\u0027Magento_Checkout/js/model/shipping-rates-validation-rules\\u0027:  \\u0027JamesonNetworks_CustomShippingRules/js/model/shipping-rates-validation-rules\\u0027\\n        },\\n        \\u0027JamesonNetworks_CustomShippingRules\\u0027 : {\\n             \\u0027Magento_Checkout/js/model/shipping-rates-validation-rules\\u0027: \\u0027Magento_Checkout/js/model/shipping-rates-validation-rules\\u0027\\n        }\\n    }\\n};\",\"type\":\"code\"},{\"content\":\"There are a couple of neat tricks in this requirejs file. Using the wildcard, we are telling M2 that every module should prefer to use \\u0027JamesonNetworks_CustomShippingRules/js/model/shipping-rates-validation-rules\\u0027. However, if we were to leave it at this, our own module would get confused when it needed to actually import the Magento_Checkout native js for our define block in our custom javascript. It would try to read that native block cyclically and throws an error. So, the second step is to map a MORE SPECIFIC selector (namely our JamesonNetworks_CustomShippingRules module) and then in that selector, we define the alias to the checkout rules that we want to use there. Because it is more specific to the module it applies to, ours will actually grab the native code and not try to cyclically grab itself again.\",\"type\":\"paragraph\"}],\"title\":\"Changes to requirejs-config.js\"},{\"contents\":[{\"content\":\"In our javascript file, we\\u0027ll pull the native functionality in and then expand on it by extending the object with underscore. In this example, we are adding two fields (street and region_id) to the observable fields for the validation to occur on.\",\"type\":\"paragraph\"},{\"content\":\"define([\\n    \\u0027underscore\\u0027,\\n    \\u0027Magento_Checkout/js/model/shipping-rates-validation-rules\\u0027\\n], function (_, Component) {\\n    \\\"use strict\\\";\\n    return _.extend({}, Component, {\\n        getObservableFields: function() {\\n            return _.union(Component.getObservableFields(), [\\u0027street\\u0027, \\u0027region_id\\u0027]);\\n        }\\n    });\\n});\",\"type\":\"code\"},{\"content\":\"Now, we are leveraging all the native code, replacing it with a very narrow piece of our own code, and we haven\\u0027t had to copy a lot of excess functionality. Easy Peezy!\",\"type\":\"paragraph\"}],\"title\":\"Example javascript file inside of my module\"}]},\"subtitle\":\"Leveraging native without incurring that maintenance overhead.\",\"title\":\"Extending Native Magento 2 Javascript Pieces\"}]');

    </script>

  </head>
  <body>
    <div class="app" id="app"></div>
  </body>
</html>
